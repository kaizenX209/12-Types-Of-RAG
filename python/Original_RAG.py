import torch
from transformers import DPRContextEncoder, DPRQuestionEncoder, DPRReader, T5ForConditionalGeneration, T5Tokenizer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import faiss
import numpy as np
import transformers
import os
import warnings
from transformers import utils
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
transformers.logging.set_verbosity_error()

# S·ª≠ d·ª•ng cached model n·∫øu c√≥
utils.use_cached_models = True

# Ho·∫∑c set offline mode
utils.offline_mode = True

### Data ###
documents = [
    # France
    "The capital of France is Paris.",
    "The Eiffel Tower is located in Paris.",
    "France is known for its wine and cheese.",
    "The French Riviera is a popular tourist destination in France.",
    "Paris is also called the City of Light.",
    "France shares borders with countries like Germany, Italy, and Spain.",
    "The Louvre in France is the world's largest art museum.",
    "Many French people enjoy visiting the USA for its iconic landmarks.",
    "France and the UK are separated by the English Channel.",
    "French cuisine is admired worldwide, especially in Vietnam due to historical ties.",
    
    # USA
    "The capital of the USA is Washington, D.C.",
    "The Statue of Liberty is located in New York City.",
    "The USA is known for its diverse culture and innovation.",
    "Hollywood in the USA is the center of the global film industry.",
    "The Grand Canyon is a natural wonder in the USA.",
    "The USA and China are two of the largest economies in the world.",
    "Jazz music originated in the USA and influenced cultures worldwide, including Japan.",
    "Many American students study abroad in countries like Germany and the UK.",
    "The USA hosts large Vietnamese communities, especially in California.",
    "Silicon Valley in the USA is a hub for technology and startups.",
    
    # UK
    "The capital of the UK is London.",
    "Big Ben is located in London.",
    "The UK is known for its tea and royal family.",
    "Oxford University in the UK is one of the oldest universities in the world.",
    "The UK and France share a tunnel called the Channel Tunnel.",
    "Shakespeare, a famous playwright, was born in the UK.",
    "The UK and the USA have a long-standing 'special relationship.'",
    "Football (soccer) originated in the UK and is popular worldwide.",
    "Many British tourists enjoy visiting Italy for its historical landmarks.",
    "The Beatles, a legendary music band, are from the UK.",
    
    # Germany
    "The capital of Germany is Berlin.",
    "The Brandenburg Gate is located in Berlin.",
    "Germany is known for its cars and beer.",
    "Oktoberfest in Germany is the world's largest beer festival.",
    "Germany is a leading country in renewable energy development.",
    "Germany and France are founding members of the European Union.",
    "Many German philosophers like Kant and Hegel shaped modern thought.",
    "German engineering is admired globally, especially in Japan.",
    "The Autobahn in Germany has sections with no speed limit.",
    "Germany and the USA collaborate on many scientific projects.",
    
    # Italy
    "The capital of Italy is Rome.",
    "The Colosseum is located in Rome.",
    "Italy is known for its pasta and pizza.",
    "Venice in Italy is famous for its canals and gondolas.",
    "Italy shares borders with France, Switzerland, and Austria.",
    "The Leaning Tower of Pisa is a famous landmark in Italy.",
    "Italian art from the Renaissance is admired in countries like the UK and France.",
    "Italy and Japan have cultural exchange programs involving fashion and design.",
    "Rome is often called the Eternal City.",
    "Many tourists from the USA visit Italy for its rich history and cuisine.",
    
    # South Korea
    "The capital of South Korea is Seoul.",
    "Gyeongbokgung Palace is located in Seoul.",
    "South Korea is known for its K-pop and technology.",
    "Samsung, a global tech giant, is headquartered in South Korea.",
    "South Korea and the USA are strong allies in East Asia.",
    "Many South Korean dishes, like kimchi, are popular worldwide, including in Vietnam.",
    "The DMZ separates South Korea from North Korea.",
    "South Korea has a rich history of hanbok, its traditional clothing.",
    "Seoul is home to many global conferences, attracting participants from the UK and Germany.",
    "South Korea and Japan have shared cultural influences despite historical tensions.",
    
    # Japan
    "The capital of Japan is Tokyo.",
    "Mount Fuji is located near Tokyo.",
    "Japan is known for its sushi and anime.",
    "The Shinkansen, or bullet train, is a technological marvel in Japan.",
    "Cherry blossoms in Japan attract visitors from countries like the USA and China.",
    "Japan has a strong cultural influence on South Korea through manga and J-pop.",
    "Tokyo hosted the Summer Olympics in 2021.",
    "Japan and Germany collaborate in automobile manufacturing.",
    "Many Japanese tourists visit Italy for its art and architecture.",
    "Japan's tea ceremonies are similar in cultural significance to China's tea traditions.",
    
    # China
    "The capital of China is Beijing.",
    "The Great Wall of China is located near Beijing.",
    "China is known for its tea and ancient history.",
    "Shanghai is one of China's most modern and populous cities.",
    "China and Vietnam share a long border and cultural exchanges.",
    "Many Chinese tourists visit France for its luxury goods.",
    "The Silk Road was an ancient trade route connecting China to Europe.",
    "China and the USA have strong trade relations.",
    "Chinese New Year is celebrated worldwide, including in the UK and Germany.",
    "The Terracotta Army is a famous archaeological site in China.",
    
    # Vietnam
    "The capital of Vietnam is Hanoi.",
    "Hoan Kiem Lake is located in Hanoi.",
    "Vietnam is known for its pho and coffee.",
    "Ha Long Bay in Vietnam is a UNESCO World Heritage Site.",
    "Vietnam shares a border with China and Laos.",
    "Many Vietnamese people immigrated to the USA after the Vietnam War.",
    "The French colonial period influenced Vietnamese architecture and cuisine.",
    "Vietnam and South Korea have strong cultural ties through entertainment and trade.",
    "The Mekong Delta is an important agricultural region in Vietnam.",
    "Vietnamese students often study abroad in countries like Japan and Australia."
]

def initialize_models():
    """
    Kh·ªüi t·∫°o c√°c m√¥ h√¨nh c·∫ßn thi·∫øt cho RAG
    
    Returns:
        tuple: G·ªìm c√°c th√†nh ph·∫ßn:
            - question_encoder: M√¥ h√¨nh m√£ h√≥a c√¢u h·ªèi
            - context_encoder: M√¥ h√¨nh m√£ h√≥a ng·ªØ c·∫£nh
            - tokenizer: Tokenizer cho T5
            - index: FAISS index ƒë·ªÉ t√¨m ki·∫øm t∆∞∆°ng ƒë·ªìng
    """
    question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
    context_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
    tokenizer = T5Tokenizer.from_pretrained("t5-base")
    index = faiss.IndexFlatIP(768)
    return question_encoder, context_encoder, tokenizer, index

def prepare_document_embeddings(documents, context_encoder, index):
    """
    Chuy·ªÉn ƒë·ªïi documents th√†nh vector embeddings v√† l∆∞u v√†o FAISS index
    
    Args:
        documents (list): Danh s√°ch c√°c vƒÉn b·∫£n
        context_encoder: M√¥ h√¨nh m√£ h√≥a ng·ªØ c·∫£nh
        index: FAISS index
        
    Returns:
        numpy.ndarray: Ma tr·∫≠n embeddings c·ªßa t·∫•t c·∫£ documents
    """
    doc_embeddings = []
    context_tokenizer = AutoTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
    
    for doc in documents:
        inputs = context_tokenizer(doc, return_tensors="pt", truncation=True, padding=True, max_length=512)
        embeddings = context_encoder(**inputs).pooler_output.detach().numpy()
        doc_embeddings.append(embeddings[0])
        index.add(embeddings)
    
    return np.vstack(doc_embeddings)

def retrieve_documents(query, question_encoder, index, documents, k=3):
    """
    T√¨m ki·∫øm c√°c documents li√™n quan nh·∫•t v·ªõi c√¢u h·ªèi
    
    Args:
        query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        question_encoder: M√¥ h√¨nh m√£ h√≥a c√¢u h·ªèi
        index: FAISS index
        documents (list): Danh s√°ch c√°c vƒÉn b·∫£n g·ªëc
        k (int): S·ªë l∆∞·ª£ng documents c·∫ßn tr·∫£ v·ªÅ
        
    Returns:
        list: Danh s√°ch c√°c documents li√™n quan nh·∫•t
    """
    # S·ª≠ d·ª•ng tokenizer ri√™ng cho question_encoder
    question_tokenizer = AutoTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
    inputs = question_tokenizer(query, return_tensors="pt", truncation=True, padding=True, max_length=512)
    
    # Encode query th√†nh vector
    query_embedding = question_encoder(**inputs).pooler_output.detach().numpy()
    
    # L·∫•y nhi·ªÅu documents h∆°n ban ƒë·∫ßu ƒë·ªÉ c√≥ context t·ªët h∆°n
    k_search = k * 2
    distances, indices = index.search(query_embedding, k_search)
    
    # S·∫Øp x·∫øp documents theo ƒë·ªô li√™n quan (similarity score)
    retrieved_docs = []
    for i, dist in zip(indices[0], distances[0]):
        retrieved_docs.append((documents[i], dist))
    
    # S·∫Øp x·∫øp theo similarity score v√† l·∫•y k documents c√≥ score cao nh·∫•t
    retrieved_docs.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in retrieved_docs[:k]]

def generate_output(query, retrieved_docs, generator, tokenizer, method="sequence"):
    """
    Sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n documents ƒë√£ retrieve
    
    Args:
        query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        retrieved_docs (list): Danh s√°ch documents ƒë√£ retrieve
        generator: M√¥ h√¨nh sinh c√¢u tr·∫£ l·ªùi
        tokenizer: Tokenizer cho m√¥ h√¨nh sinh
        method (str): Ph∆∞∆°ng ph√°p RAG ("sequence" ho·∫∑c "token")
        
    Returns:
        str: C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c sinh ra
    """
    context = " ".join(retrieved_docs) if method == "sequence" else retrieved_docs[0]
    
    # ƒê·ªãnh d·∫°ng input cho T5
    prompt = f"question: {query} context: {context}"
    
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    outputs = generator.generate(
        **inputs,
        max_length=128,
        min_length=20,
        num_beams=3,
        do_sample=False,  # T·∫Øt sampling ƒë·ªÉ c√≥ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n
        temperature=1.0,
        early_stopping=True
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    """
    H√†m ch√≠nh th·ª±c thi pipeline RAG
    """
    print("\nü§ñ Initializing RAG pipeline...")
    
    question_encoder, context_encoder, tokenizer, index = initialize_models()
    
    print("üìö Preparing document embeddings...")
    doc_embeddings = prepare_document_embeddings(documents, context_encoder, index)
    
    # Kh·ªüi t·∫°o T5 thay v√¨ BART
    generator = T5ForConditionalGeneration.from_pretrained("t5-base")
    
    query = "I am learning about Vietnamese culture."
    print(f"\n‚ùì Question: {query}")
    
    retrieved_docs = retrieve_documents(query, question_encoder, index, documents, k=3)
    print("\nüìÑ Retrieved documents:")
    for i, doc in enumerate(retrieved_docs, 1):
        print(f"{i}. {doc}")
    
    print("\nüîç RAG-Sequence Response:")
    response_sequence = generate_output(query, retrieved_docs, generator, tokenizer, method="sequence")
    print(response_sequence)
    
    print("\nüîç RAG-Token Response:")
    response_token = generate_output(query, retrieved_docs, generator, tokenizer, method="token")
    print(response_token)

if __name__ == "__main__":
    main()
